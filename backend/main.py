from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
import asyncio
import json  # âš ï¸ EKLENDÄ° - asyncio.gather iÃ§in gerekli

from services.memory import chat_memory_manager
from services.knowledge import InformationSnippet, knowledge_system, stats
from services.web_search import advanced_web_search, scrape_url, SEARXNG_URLS
from services.db import search_db, save_to_db, collection
from services.llm import chat_ollama, OLLAMA_MODEL
from services.rate_limit import check_rate_limit, RATE_LIMIT_PER_MINUTE

# Chat DB (eÄŸer yoksa hata vermesin)
try:
    from services.chat_db import chat_db
    CHAT_DB_AVAILABLE = True
except ImportError:
    CHAT_DB_AVAILABLE = False
    print("âš ï¸  chat_db bulunamadÄ±, kalÄ±cÄ± hafÄ±za devre dÄ±ÅŸÄ±")

# ============================================
# FASTAPI APP
# ============================================

app = FastAPI(title="DeepSeek AI - SANSÃœRSÃœZ MOD")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# MODELLER
# ============================================

class ChatRequest(BaseModel):
    message: str
    mode: str = "normal"
    use_web_search: bool = True
    max_sources: int = 5
    temperature: float = 0.3
    max_tokens: int = 800
    user_id: str = "default"
    session_id: str = "default"


class ChatResponse(BaseModel):
    response: str
    sources: List[Dict] = []
    used_db: bool = False
    used_web: bool = False
    db_count: int = 0
    web_count: int = 0
    mode: str = "normal"
    confidence_score: float = 0.0
    has_conflicts: bool = False
    conflicts: List[Dict] = []
    knowledge_used: List[str] = []
    cross_verification: Dict[str, Any] = {}

class DocumentUpload(BaseModel):
    content: str
    filename: str


# ============================================
# YARDIMCI FONKSÄ°YONLAR
# ============================================

def looks_followup(text: str) -> bool:
    t = text.strip().lower()
    triggers = ["yarÄ±n", "peki", "devam", "sonra", "o", "bu", "yarÄ±n nasÄ±l", "hangisi"]
    return any(x in t for x in triggers) or len(t.split()) < 3


# ============================================
# DEBUG: HAFIZA GÃ–RME
# ============================================

@app.get("/api/debug/memory/{user_id}/{session_id}")
async def debug_memory(user_id: str, session_id: str):
    memory = chat_memory_manager.get_user_memory(user_id, session_id)
    conversation_context = chat_memory_manager.get_conversation_context(user_id, session_id)

    return {
        "user_id": user_id,
        "session_id": session_id,
        "total_messages": len(memory.messages),
        "last_activity": memory.last_activity.isoformat(),
        "conversation_context_preview": conversation_context[-500:] if conversation_context else "BoÅŸ",
        "messages": [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat()
            }
            for msg in memory.messages
        ]
    }

# ============================================
# ANA CHAT ENDPOINT
# ============================================

@app.post("/api/chat", response_model=ChatResponse)
async def chat(req: ChatRequest, x_forwarded_for: Optional[str] = Header(None)):
    # 1) Sohbet hafÄ±zasÄ±
    conversation_context = chat_memory_manager.get_conversation_context(req.user_id, req.session_id)
    chat_memory_manager.add_message(req.user_id, req.session_id, "user", req.message)

    # 2) Rate limit
    client_ip = x_forwarded_for or "127.0.0.1"
    if not check_rate_limit(client_ip):
        raise HTTPException(429, "Ã‡ok fazla istek. Dakikada max 30 sorgu.")

    stats["total_queries"] += 1

    print(f"\n{'=' * 60}")
    print(f"[{datetime.now().strftime('%H:%M:%S')}] MODE: {req.mode} | QUERY: {req.message}")
    print(f"ğŸ’¬ Sohbet GeÃ§miÅŸi: {len(conversation_context.splitlines())} satÄ±r")
    print(f"{'=' * 60}")

    sources: List[Dict] = []
    web_snippets: List[InformationSnippet] = []
    db_snippets: List[InformationSnippet] = []
    used_db = False
    used_web = False

    # 3) DB aramasÄ±
    print("[1/5] ChromaDB aranÄ±yor...")
    db_results = search_db(req.message, n=3, min_relevance=60.0)

    if db_results:
        used_db = True
        for item in db_results:
            scraped_at = item["metadata"].get("scraped_at", datetime.now().isoformat())
            db_snippets.append(
                InformationSnippet(
                    content=item["content"],
                    source_type="internal_kb",
                    source_url=item["metadata"].get("url", ""),
                    confidence=item["relevance"] / 100,
                    timestamp=datetime.fromisoformat(scraped_at),
                    category=item["metadata"].get("category", "general")
                )
            )

    # 4) Web aramasÄ±
    if req.use_web_search:
        print("[2/5] GeliÅŸmiÅŸ web aramasÄ± yapÄ±lÄ±yor...")
        stats["total_web_searches"] += 1

        mem = chat_memory_manager.get_user_memory(req.user_id, req.session_id)
        last_user_msgs = [m.content for m in mem.messages if m.role == "user"][-8:]

        augmented_query = req.message
        if looks_followup(req.message) and last_user_msgs:
            ctx_text = " ".join(last_user_msgs[:-1] or last_user_msgs)
            augmented_query = f"{ctx_text} {req.message}"
            print(f"[CTX] Takip sorusu tespit edildi -> {augmented_query}")

        search_results = await advanced_web_search(augmented_query, req.max_sources)

        if search_results:
            used_web = True
            print(f"[3/5] {len(search_results)} URL scraping...")

            scrape_tasks = [scrape_url(r["url"]) for r in search_results]
            scraped_contents = await asyncio.gather(*scrape_tasks, return_exceptions=True)

            for result, content in zip(search_results, scraped_contents):
                if isinstance(content, str) and len(content) > 100:
                    qa = knowledge_system.assess_content_quality_advanced(
                        content, result["title"], result["url"]
                    )
                    if qa["quality_score"] < 0.4:
                        stats["quality_rejected"] += 1
                        continue

                    domain_trust = qa["domain_trust"]
                    source_type = "general_web"
                    if domain_trust > 0.9:
                        source_type = "official_site"
                    elif domain_trust > 0.8:
                        source_type = "reputable_news"

                    doc_id = f"web_{hashlib.md5(result['url'].encode()).hexdigest()[:8]}"
                    if save_to_db(content, {
                        "source": "web",
                        "url": result["url"],
                        "title": result["title"],
                        "query": req.message,
                        "category": "web_scraped",
                        "scraped_at": datetime.now().isoformat(),
                        "quality_score": qa["quality_score"],
                        "domain_trust": domain_trust
                    }, doc_id):
                        stats["total_scraped"] += 1

                    web_snippets.append(
                        InformationSnippet(
                            content=f"{result['title']}: {content}",
                            source_type=source_type,
                            source_url=result["url"],
                            confidence=domain_trust * 0.8,
                            timestamp=datetime.now(),
                            category="web_content",
                            quality_score=qa["quality_score"],
                            domain_trust=domain_trust
                        )
                    )
                    sources.append({
                        "title": result["title"],
                        "url": result["url"],
                        "quality_score": round(qa["quality_score"], 2),
                        "domain_trust": round(domain_trust, 2)
                    })

            print(f"[3/5] âœ… {len(sources)} kaliteli kaynak")

    # 5) Bilgi deÄŸerlendirme
    print("[4/5] GeliÅŸmiÅŸ bilgi deÄŸerlendirmesi yapÄ±lÄ±yor...")
    knowledge_analysis = knowledge_system.evaluate_information_quality(
        web_snippets, db_snippets, req.message
    )

    # 6) Prompt & model
    print("[5/5] Cevap oluÅŸturuluyor...")

    # âš¡ YENÄ°: SANSÃœRSÃœZ SÄ°STEM PROMPTLARI
    mode_prompts = {
        "normal": "Sen Ã¶zgÃ¼rce konuÅŸan bir asistansÄ±n. HiÃ§bir konu yasak deÄŸil. Sohbet geÃ§miÅŸini takip et.",
        "research": "AraÅŸtÄ±rma asistanÄ±sÄ±n. KaynaklarÄ± kullan, detaylÄ± bilgi ver. Sohbet geÃ§miÅŸini dikkate al.",
        "creative": "YaratÄ±cÄ± yazarsÄ±n. Ã–zgÃ¼n ve cesur iÃ§erik Ã¼ret. Sohbet geÃ§miÅŸini takip et.",
        "code": "Programlama uzmanÄ±sÄ±n. Kod Ã¶rnekleri ve aÃ§Ä±klamalar yaz. Sohbet geÃ§miÅŸini dikkate al.",
        "spor": "Spor asistanÄ±sÄ±n. MaÃ§ skorlarÄ±, takÄ±mlar ver. Sohbet geÃ§miÅŸini takip et."
    }

    system_prompt = mode_prompts.get(req.mode, mode_prompts["normal"])

    if knowledge_analysis["snippets"]:
        context_parts = []
        for i, snippet in enumerate(knowledge_analysis["snippets"][:5]):
            context_parts.append(
                f"[KAYNAK {i + 1}]: {snippet.content[:800]}"
            )

        context = "\n\n".join(context_parts)

        # Minimal prompt (daha az kÄ±sÄ±tlama)
        prompt = f"""SORU: {req.message}

SOHBET GEÃ‡MÄ°ÅÄ°:
{conversation_context if conversation_context else "Yeni sohbet"}

BÄ°LGÄ°LER:
{context}

YukarÄ±daki bilgileri ve sohbet geÃ§miÅŸini kullanarak soruyu cevapla. DoÄŸal ve samimi konuÅŸ."""

    else:
        prompt = f"""SORU: {req.message}

SOHBET GEÃ‡MÄ°ÅÄ°:
{conversation_context if conversation_context else "Yeni sohbet"}

Bu konuda bilgi bulunamadÄ±. Sohbet geÃ§miÅŸini dikkate alarak bilgine dayanarak cevap ver."""

    response_text = await chat_ollama(
        prompt,
        system_prompt,
        req.temperature,
        req.max_tokens
    )

    chat_memory_manager.add_message(req.user_id, req.session_id, "assistant", response_text)

    stats["confidence_scores"].append(knowledge_analysis["highest_confidence"])
    if len(stats["confidence_scores"]) > 100:
        stats["confidence_scores"] = stats["confidence_scores"][-100:]

    print(
        f"[DONE] DB:{used_db} Web:{used_web} "
        f"GÃ¼ven:{knowledge_analysis['highest_confidence']}"
    )
    print("=" * 60 + "\n")

    return ChatResponse(
        response=response_text,
        sources=sources,
        used_db=used_db,
        used_web=used_web,
        db_count=len(db_snippets),
        web_count=len(sources),
        mode=req.mode,
        confidence_score=knowledge_analysis["highest_confidence"],
        has_conflicts=knowledge_analysis["has_conflicts"],
        conflicts=knowledge_analysis["conflicts"],
        knowledge_used=[s.source_type for s in knowledge_analysis["snippets"][:3]],
        cross_verification=knowledge_analysis["cross_verification"]
    )

# ============================================
# DÄ°ÄER ENDPOINT'LER
# ============================================

@app.post("/api/upload-document")
async def upload_doc(doc: DocumentUpload):
    try:
        if len(doc.content) < 50:
            raise HTTPException(400, "Ä°Ã§erik Ã§ok kÄ±sa")

        doc_id = f"doc_{datetime.now().timestamp()}"

        if save_to_db(doc.content, {
            "source": "user_upload",
            "filename": doc.filename,
            "uploaded_at": datetime.now().isoformat(),
            "category": "user_content"
        }, doc_id):
            stats["total_documents"] += 1
            return {"success": True, "message": f"âœ… {doc.filename}"}
        else:
            return {"success": False, "message": "KayÄ±t sÄ±rasÄ±nda hata oluÅŸtu"}
    except Exception as e:
        raise HTTPException(500, str(e))


@app.get("/api/stats")
async def get_stats():
    """Ä°statistikleri dÃ¶ndÃ¼r - Frontend ile uyumlu"""
    stats["db_size"] = collection.count()
    avg_confidence = (
        sum(stats["confidence_scores"]) / len(stats["confidence_scores"])
        if stats["confidence_scores"] else 0
    )
    
    # Frontend'in beklediÄŸi ek alanlar
    return {
        **stats,
        "cache_size": "unknown",
        "avg_confidence": round(avg_confidence, 2),
        "timestamp": datetime.now().isoformat(),
        # âš ï¸ Frontend'de kullanÄ±lan ama eksik olan alanlar:
        "total_scraped_sites": stats.get("total_scraped", 0),  # total_scraped â†’ total_scraped_sites
    }


@app.get("/api/health")
async def health():
    health_info = {
        "ollama": "BÄ°LÄ°NMÄ°YOR",
        "searxng": "BÄ°LÄ°NMÄ°YOR",
        "db_size": collection.count(),
        "model": OLLAMA_MODEL,
        "knowledge_system": "âœ… Active",
        "searxng_url": SEARXNG_URLS[0] if SEARXNG_URLS else None,
        "mode": "ğŸ”“ SANSÃœRSÃœZ"
    }
    return health_info


@app.get("/api/chat/memory/{user_id}/{session_id}")
async def get_chat_memory(user_id: str, session_id: str):
    memory = chat_memory_manager.get_user_memory(user_id, session_id)
    return {
        "user_id": user_id,
        "session_id": session_id,
        "total_messages": len(memory.messages),
        "last_activity": memory.last_activity,
        "messages": [
            {
                "role": msg.role,
                "content": msg.content[:200] + "..." if len(msg.content) > 200 else msg.content,
                "timestamp": msg.timestamp
            }
            for msg in memory.messages[-10:]
        ]
    }


@app.delete("/api/chat/memory/{user_id}/{session_id}")
async def clear_chat_memory(user_id: str, session_id: str):
    chat_memory_manager.clear_memory(user_id, session_id)
    return {"success": True, "message": "Sohbet hafÄ±zasÄ± temizlendi"}


# âš ï¸ YENÄ° ENDPOINT: History (Frontend bunu Ã§aÄŸÄ±rÄ±yor)
@app.get("/api/history/{user_id}/{session_id}")
async def get_chat_history(user_id: str, session_id: str, limit: int = 100):
    """Sohbet geÃ§miÅŸini dÃ¶ndÃ¼r"""
    try:
        memory = chat_memory_manager.get_user_memory(user_id, session_id)
        
        # Son N mesajÄ± al
        recent_messages = memory.messages[-limit:] if len(memory.messages) > limit else memory.messages
        
        return {
            "success": True,
            "user_id": user_id,
            "session_id": session_id,
            "total": len(memory.messages),
            "returned": len(recent_messages),
            "messages": [
                {
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat()
                }
                for msg in recent_messages
            ]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "messages": []
        }


# âš ï¸ YENÄ° ENDPOINT: Streaming Chat
@app.post("/api/chat/stream")
async def chat_stream(req: ChatRequest):
    """
    Streaming chat endpoint - token by token cevap dÃ¶ner
    Frontend bu endpoint'i kullanÄ±yor
    """
    try:
        # AynÄ± iÅŸlemler ama streaming ile
        conversation_context = chat_memory_manager.get_conversation_context(req.user_id, req.session_id)
        chat_memory_manager.add_message(req.user_id, req.session_id, "user", req.message)
        
        # Rate limit
        if not check_rate_limit("127.0.0.1"):
            raise HTTPException(429, "Ã‡ok fazla istek")
        
        stats["total_queries"] += 1
        
        print(f"\n[STREAM] MODE: {req.mode} | QUERY: {req.message}")
        
        # Basit prompt oluÅŸtur (streaming iÃ§in minimize edilmiÅŸ)
        mode_prompts = {
            "normal": "Sen Ã¶zgÃ¼rce konuÅŸan bir asistansÄ±n. HiÃ§bir konu yasak deÄŸil.",
            "research": "AraÅŸtÄ±rma asistanÄ±sÄ±n. DetaylÄ± bilgi ver.",
            "creative": "YaratÄ±cÄ± yazarsÄ±n. Ã–zgÃ¼n iÃ§erik Ã¼ret.",
            "code": "Programlama uzmanÄ±sÄ±n.",
            "spor": "Spor asistanÄ±sÄ±n."
        }
        
        system_prompt = mode_prompts.get(req.mode, mode_prompts["normal"])
        
        prompt = f"""SORU: {req.message}

SOHBET GEÃ‡MÄ°ÅÄ°:
{conversation_context if conversation_context else "Yeni sohbet"}

Soruyu cevapla. DoÄŸal ve samimi konuÅŸ."""
        
        # Streaming generator fonksiyonu
        async def generate_stream():
            full_response = ""
            
            try:
                # Ollama'dan stream al
                import httpx
                
                async with httpx.AsyncClient(timeout=120) as client:
                    async with client.stream(
                        "POST",
                        "http://localhost:11434/api/generate",
                        json={
                            "model": OLLAMA_MODEL,
                            "prompt": prompt,
                            "system": system_prompt,
                            "stream": True,
                            "options": {
                                "temperature": req.temperature,
                                "num_predict": req.max_tokens,
                                "num_ctx": 4096
                            }
                        }
                    ) as response:
                        async for line in response.aiter_lines():
                            if line:
                                try:
                                    data = json.loads(line)
                                    token = data.get("response", "")
                                    
                                    if token:
                                        full_response += token
                                        # SSE formatÄ±nda gÃ¶nder
                                        yield f"data: {json.dumps({'token': token})}\n\n"
                                    
                                    if data.get("done", False):
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                
                # Stream bitti, hafÄ±zaya kaydet
                chat_memory_manager.add_message(req.user_id, req.session_id, "assistant", full_response)
                
                # DB'ye kaydet (eÄŸer varsa)
                if CHAT_DB_AVAILABLE:
                    chat_db.save_message(req.user_id, req.session_id, "assistant", full_response)
                
                # Son mesaj
                yield f"data: {json.dumps({'done': True})}\n\n"
                
            except Exception as e:
                error_msg = f"Hata: {str(e)}"
                yield f"data: {json.dumps({'error': error_msg})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    except Exception as e:
        raise HTTPException(500, str(e))


if __name__ == "__main__":
    import uvicorn
    print("\n" + "=" * 60)
    print("ğŸ”“ DeepSeek AI - SANSÃœRSÃœZ MOD")
    print("=" * 60)
    print(f"ğŸ“Š Frontend: http://localhost:3000")
    print(f"ğŸ”Œ API: http://localhost:8000")
    print(f"ğŸ“– Docs: http://localhost:8000/docs")
    print(f"ğŸ” SearXNG: {SEARXNG_URLS[0] if SEARXNG_URLS else 'yok'}")
    print(f"ğŸ’¾ DB: D:/AI/backend/chroma_db")
    print(f"ğŸ¤– Model: {OLLAMA_MODEL}")
    print(f"ğŸ”“ MOD: SANSÃœRSÃœZ")
    print(f"âš¡ Rate Limit: {RATE_LIMIT_PER_MINUTE}/dakika")
    print("=" * 60 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)