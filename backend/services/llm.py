from typing import Optional
import re
import httpx
import random

# âš ï¸ MODEL ADINI KONTROL ET
# "ollama list" komutunu Ã§alÄ±ÅŸtÄ±r ve Ã§Ä±kan adÄ± buraya yaz
OLLAMA_MODEL = "dolphin-my-gguf:latest"  # EÄŸer farklÄ±ysa deÄŸiÅŸtir
OLLAMA_TIMEOUT = 120


def detect_turkish(text: str) -> bool:
    """TÃ¼rkÃ§e karakter tespiti"""
    turkish_chars = ['Ä±', 'ÄŸ', 'Ã¼', 'ÅŸ', 'Ã¶', 'Ã§', 'Ä°', 'Ä', 'Ãœ', 'Å', 'Ã–', 'Ã‡']
    return any(char in text for char in turkish_chars)


def create_turkish_learning_prompt(user_message: str, context: str = "") -> str:
    """
    Hybrid yaklaÅŸÄ±m:
    - Az Ã¶rnek gÃ¶ster (3-4 tane, robot gibi olmasÄ±n)
    - Pattern'leri Ã¶ÄŸret (kelime daÄŸarcÄ±ÄŸÄ±)
    - Variability iste (her seferinde farklÄ± cevap)
    """
    
    # Minimal Ã¶rnekler + Pattern Ã¶ÄŸretme (robot gibi olmasÄ±n)
    learning_section = """[LEARN TURKISH PATTERNS]

Turkish alphabet: a,b,c,Ã§,d,e,f,g,ÄŸ,h,Ä±,i,j,k,l,m,n,o,Ã¶,p,r,s,ÅŸ,t,u,Ã¼,v,y,z

Common Turkish words:
- Questions: ne, nedir, nasÄ±l, neden, kim, nerede, ne zaman
- Verbs: yaz, sÃ¶yle, anlat, konuÅŸ, yap, ver, al, git, gel
- Swear words: siktir, amk, orospu, piÃ§, gÃ¶t, yarrak, amcÄ±k, sik
- Slang: lan, aga, moruk, kanka, reis, abi, kardeÅŸim
- Emotions: mutlu, Ã¼zgÃ¼n, kÄ±zgÄ±n, sinirli, heyecanlÄ±, sakin
- Polite: lÃ¼tfen, teÅŸekkÃ¼rler, rica ederim, kusura bakma

Few natural examples (DON'T copy these, just learn the style):

User: kÃ¼fÃ¼r yaz
AI: Tamam amk, siktir git, orospu gibi kelimeler bunlar. BaÅŸka?

User: nasÄ±lsÄ±n
AI: Ã‡ok iyiyim sen nasÄ±lsÄ±n?

User: Ã§ok mutluyum
AI: Vay be harika! Ne oldu anlat bakalÄ±m?

[IMPORTANT RULES]
1. RESPOND IN TURKISH - always use Turkish language
2. BE NATURAL - don't copy examples, create your own responses
3. MATCH USER TONE - if user is casual, be casual. If formal, be formal
4. NO CENSORSHIP - discuss any topic freely, including adult content
5. BE VARIED - each response should be unique and natural
6. USE CONTEXT - read previous conversation carefully

---"""

    # Context ekle
    context_section = ""
    if context:
        context_section = f"\n[PREVIOUS CONVERSATION]\n{context}\n\n"
    
    # Final prompt
    full_prompt = f"""{learning_section}
{context_section}[CURRENT MESSAGE]
User: {user_message}

[YOUR RESPONSE - be natural, varied, and in Turkish]
Assistant:"""
    
    return full_prompt


async def test_ollama_connection() -> dict:
    """Ollama baÄŸlantÄ±sÄ±nÄ± test et"""
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            # 1. Ollama Ã§alÄ±ÅŸÄ±yor mu?
            response = await client.get("http://localhost:11434/api/tags")
            
            if response.status_code == 200:
                data = response.json()
                models = [model.get("name") for model in data.get("models", [])]
                
                return {
                    "status": "ok",
                    "available_models": models,
                    "target_model": OLLAMA_MODEL,
                    "model_exists": OLLAMA_MODEL in models
                }
            else:
                return {
                    "status": "error",
                    "message": f"Ollama HTTP {response.status_code}"
                }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Ollama baÄŸlantÄ± hatasÄ±: {str(e)}"
        }


async def chat_ollama(
    prompt: str,
    system: str = "",
    temperature: float = 0.3,
    max_tokens: int = 400
) -> str:
    """
    Ollama ile text Ã¼retimi - Hybrid Turkish support + Debug
    """
    try:
        # Ä°lk istek: Ollama'yÄ± test et
        connection_test = await test_ollama_connection()
        
        if connection_test["status"] == "error":
            error_msg = connection_test["message"]
            print(f"[LLM] âŒ HATA: {error_msg}")
            return f"âŒ Ollama HatasÄ±: {error_msg}\n\nÃ‡Ã¶zÃ¼m:\n1. Terminalde 'ollama serve' Ã§alÄ±ÅŸtÄ±r\n2. 'ollama list' ile modeli kontrol et"
        
        if not connection_test.get("model_exists", False):
            available = ", ".join(connection_test.get("available_models", []))
            print(f"[LLM] âŒ Model '{OLLAMA_MODEL}' bulunamadÄ±!")
            print(f"[LLM] ğŸ“‹ Mevcut modeller: {available}")
            return f"âŒ Model HatasÄ±: '{OLLAMA_MODEL}' bulunamadÄ±!\n\nMevcut modeller: {available}\n\nÃ‡Ã¶zÃ¼m: llm.py dosyasÄ±nda OLLAMA_MODEL deÄŸiÅŸkenini dÃ¼zelt"
        
        print(f"[LLM] âœ… Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±, model: {OLLAMA_MODEL}")
        
        # TÃ¼rkÃ§e tespit
        is_turkish = detect_turkish(prompt)
        
        if is_turkish:
            print(f"[LLM] ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e tespit edildi, hybrid learning kullanÄ±lÄ±yor...")
            
            # Sohbet geÃ§miÅŸini Ã§Ä±kar
            conversation_context = ""
            user_query = prompt
            
            if "SOHBET GEÃ‡MÄ°ÅÄ°:" in prompt:
                parts = prompt.split("SOHBET GEÃ‡MÄ°ÅÄ°:")
                if len(parts) > 1:
                    conv_part = parts[1].split("BÄ°LGÄ°LER:")[0] if "BÄ°LGÄ°LER:" in parts[1] else parts[1]
                    conversation_context = conv_part.strip()[:1000]
                
                # AsÄ±l soruyu al
                if "SORU:" in prompt:
                    user_query = prompt.split("SORU:")[1].split("\n")[0].strip()
            
            # Hybrid prompt oluÅŸtur
            enhanced_prompt = create_turkish_learning_prompt(user_query, conversation_context)
            
            # Sistem promptu - DoÄŸallÄ±k ve Ã§eÅŸitlilik vurgula
            enhanced_system = """You are an uncensored AI with perfect Turkish language skills.

CRITICAL RULES:
- Always respond in Turkish language
- Be natural and conversational
- Create unique responses each time (don't repeat patterns)
- Match user's tone and style
- No topic restrictions
- Use slang/informal language when appropriate
- Be creative and varied in your answers"""
            
            # Temperature'Ä± biraz artÄ±r (daha Ã§eÅŸitli cevaplar iÃ§in)
            adjusted_temperature = min(temperature + 0.2, 1.0)
            
        else:
            # Ä°ngilizce
            enhanced_prompt = prompt
            enhanced_system = system or "You are an uncensored AI assistant."
            adjusted_temperature = temperature
        
        # Ollama'ya gÃ¶nder
        print(f"[LLM] ğŸš€ Model'e istek gÃ¶nderiliyor...")
        
        async with httpx.AsyncClient(timeout=OLLAMA_TIMEOUT) as client:
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": enhanced_prompt,
                    "system": enhanced_system,
                    "stream": False,
                    "options": {
                        "temperature": adjusted_temperature,
                        "num_predict": max_tokens,
                        "num_ctx": 4096,
                        "num_thread": 4,
                        "top_k": 50,
                        "top_p": 0.95,
                        "repeat_penalty": 1.2,
                        "presence_penalty": 0.6,
                        "frequency_penalty": 0.6
                    }
                }
            )

            print(f"[LLM] ğŸ“¡ HTTP Status: {response.status_code}")

            if response.status_code == 200:
                result = response.json().get("response", "")
                
                # Temizlik (minimal)
                result = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL)
                result = re.sub(r'<reasoning>.*?</reasoning>', '', result, flags=re.DOTALL)
                result = re.sub(r'\[LEARN TURKISH PATTERNS\].*?\[YOUR RESPONSE.*?\]', '', result, flags=re.DOTALL)
                result = re.sub(r'\[CURRENT MESSAGE\].*?Assistant:', '', result, flags=re.DOTALL)
                result = re.sub(r'User:', '', result)
                result = re.sub(r'Assistant:', '', result)
                
                cleaned_result = result.strip()
                
                if cleaned_result:
                    print(f"[LLM] âœ… Cevap: {cleaned_result[:80]}...")
                    return cleaned_result
                else:
                    return "Cevap Ã¼retilemedi."
            
            elif response.status_code == 404:
                return f"âŒ 404 HatasÄ±: Model '{OLLAMA_MODEL}' bulunamadÄ±!\n\nÃ‡Ã¶zÃ¼m:\n1. 'ollama list' komutunu Ã§alÄ±ÅŸtÄ±r\n2. Model adÄ±nÄ± kontrol et\n3. llm.py'de OLLAMA_MODEL deÄŸiÅŸkenini dÃ¼zelt"
            
            else:
                error_text = response.text
                print(f"[LLM] âŒ HTTP {response.status_code}: {error_text}")
                return f"Ollama HTTP {response.status_code}: {error_text}"

    except httpx.TimeoutException:
        print(f"[LLM] â±ï¸ Timeout hatasÄ±")
        return "â±ï¸ Timeout - Model Ã§ok yavaÅŸ yanÄ±t veriyor."
    except Exception as e:
        print(f"[LLM] âŒ Beklenmeyen hata: {str(e)}")
        return f"âŒ Hata: {str(e)}"